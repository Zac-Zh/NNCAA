# Credibility-Based Adversarial Training Framework for Neural Networks

This project implements an adversarial training framework based on neuron credibility, distinguishing High-Credibility Neurons (HNs) and Low-Credibility Neurons (LNs) to enhance model generalization and robustness.

## Project Structure

```
├── main.py                 # Entry point for MNIST experiments
├── main_cifar.py           # Entry point for CIFAR-10 experiments
├── models/                 # Model definitions
│   ├── baseline_cnn.py     # Baseline CNN for MNIST
│   ├── cifar_cnn.py        # Baseline CNN for CIFAR-10
│   ├── hn_classifier.py    # Classifier based on high-credibility neurons
│   └── ln_generator.py     # Generator based on low-credibility neurons
├── utils/                  # Utility functions
│   ├── credibility.py      # Neuron credibility computation (causal inference-based)
│   ├── data_loader.py      # MNIST data loading and preprocessing
│   ├── cifar_loader.py     # CIFAR-10 data loading and preprocessing
│   └── visualization.py    # Visualization tools
└── requirements.txt        # Project dependencies

```

## Experimental Pipeline

1. **Pre-training**: Train a baseline CNN for classification (MNIST or CIFAR-10).
2. **Credibility Estimation**：Compute neuron credibility using causal inference methods.
3. **Network Partitioning**：Divide neurons into High-Credibility Neurons (HNs) and Low-Credibility Neurons (LNs).
4. **Adversarial Training**：
      - Use HNs to build the Classifier
      
      - Use LNs to build the Adversarial Generator
      
      - Alternately train both networks in an adversarial manner

## How to Run

```bash
# Install dependencies
pip install -r requirements.txt

# Run MNIST experiment
python main.py

# Run CIFAR-10 experiment (with visualization)
python main_cifar.py --visualize --save-dir ./results_cifar

```

## Visualization Tools

This project provides several tools for analyzing adversarial examples.

1. **Adversarial Sample Visualization**:Display original images, perturbations, and generated adversarial examples
2. **Decision Boundary Distance Analysis**: Measure how close adversarial examples are to decision boundaries
3. **Feature Space Visualization**: Use t-SNE to visualize sample distribution in feature space

## Evaluation Metrics

   - Clean Accuracy: Classification accuracy of HNs on clean test sets
   
   - Adversarial Accuracy: Accuracy of HNs on samples generated by LNs
   
   - Generalization Ability: Robustness of HNs to noise or adversarial perturbations
   
   - Attack Success Rate: Percentage of LN-generated samples that cause HNs to misclassify
